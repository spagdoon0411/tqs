{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Supervised Optimizer\n",
    "\n",
    "From start to finish, on pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from Hamiltonian import Ising\n",
    "from model import TransformerModel\n",
    "from optimizer_supervised import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes:\n",
      " [[ 4]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [10]\n",
      " [12]\n",
      " [14]\n",
      " [16]\n",
      " [18]\n",
      " [20]]\n",
      "Hamiltonians: [<Hamiltonian.Ising object at 0x7ce28fd92420>, <Hamiltonian.Ising object at 0x7ce444400500>, <Hamiltonian.Ising object at 0x7ce444402d50>, <Hamiltonian.Ising object at 0x7ce28f9ad250>, <Hamiltonian.Ising object at 0x7ce28f9ac800>, <Hamiltonian.Ising object at 0x7ce28f9ad2b0>, <Hamiltonian.Ising object at 0x7ce28f9ad310>, <Hamiltonian.Ising object at 0x7ce28f9ad370>, <Hamiltonian.Ising object at 0x7ce28f9ad3d0>]\n",
      "Dimensions of parameter space: 1\n",
      "Number of units in a feedforward layer: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/Projects/tqs/Hamiltonian_utils.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  system_size = torch.tensor(system_size, dtype=torch.int64).reshape(-1)\n"
     ]
    }
   ],
   "source": [
    "system_sizes = np.arange(4, 21, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 10000\n",
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False\n",
    "\n",
    "print(\"Sizes:\\n\", system_sizes)\n",
    "print(\"Hamiltonians:\", Hamiltonians)\n",
    "print(\"Dimensions of parameter space:\", param_dim)\n",
    "print(\"Number of units in a feedforward layer:\", n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "testmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(testmodel, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      2\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      3\u001b[0m     param_range\u001b[38;5;241m=\u001b[39mparam_range,\n\u001b[1;32m      4\u001b[0m     param_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised.py:286\u001b[0m, in \u001b[0;36mOptimizer.train\u001b[0;34m(self, epochs, param_range, param_step, ensemble_id, start_iter)\u001b[0m\n\u001b[1;32m    283\u001b[0m ham_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    284\u001b[0m system_size \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39msystem_size\n\u001b[0;32m--> 286\u001b[0m points_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_parameter_points(\n\u001b[1;32m    287\u001b[0m     param_range, param_step\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points_generator:\n\u001b[1;32m    291\u001b[0m     point_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Projects/tqs/optimizer_supervised.py:231\u001b[0m, in \u001b[0;36mOptimizer.generate_parameter_points\u001b[0;34m(self, parameter_ranges, step_sizes, distribution)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampling using a custom distribution is not implemented yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Every possible individual parameter value for each parameter, in order\u001b[39;00m\n\u001b[1;32m    229\u001b[0m parameter_ranges \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_parameter_range(start\u001b[38;5;241m.\u001b[39mitem(), end\u001b[38;5;241m.\u001b[39mitem(), step\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (start, end), step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameter_ranges, step_sizes)\n\u001b[1;32m    232\u001b[0m ]\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mparameter_ranges)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "param_range=torch.tensor([[0.5], [1.5]])\n",
    "param_step=torch.tensor([0.1])\n",
    "\n",
    "opt.train(\n",
    "    epochs=10,\n",
    "    param_range=param_range,\n",
    "    param_step=t0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
