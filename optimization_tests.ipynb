{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer_supervised import Optimizer\n",
    "from model import TransformerModel\n",
    "from Hamiltonian import Ising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    # Set the device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU\")\n",
    "\n",
    "# Example usage: Move a tensor to the selected device\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(\n",
    "    torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: [[ 8]\n",
      " [10]\n",
      " [12]\n",
      " [14]\n",
      " [16]\n",
      " [18]\n",
      " [20]]\n",
      "Hamiltonians: [<Hamiltonian.Ising object at 0x7cf4f035d400>, <Hamiltonian.Ising object at 0x7cf50f869460>, <Hamiltonian.Ising object at 0x7cf50fa2fc80>, <Hamiltonian.Ising object at 0x7cf4f035eba0>, <Hamiltonian.Ising object at 0x7cf50f86b7a0>, <Hamiltonian.Ising object at 0x7cf4f035fe90>, <Hamiltonian.Ising object at 0x7cf4f035d2e0>]\n",
      "Param dim: 1\n"
     ]
    }
   ],
   "source": [
    "system_sizes = np.arange(8, 21, 2).reshape(-1, 1)\n",
    "Hamiltonians = [Ising(size, periodic=True) for size in system_sizes]\n",
    "param_dim = Hamiltonians[0].param_dim\n",
    "embedding_size = 32\n",
    "n_head = 8\n",
    "n_hid = embedding_size\n",
    "n_layers = 8\n",
    "dropout = 0\n",
    "minibatch = 10000\n",
    "\n",
    "print(\"Sizes:\", system_sizes)\n",
    "print(\"Hamiltonians:\", Hamiltonians)\n",
    "print(\"Param dim:\", param_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    system_sizes,\n",
    "    param_dim,\n",
    "    embedding_size,\n",
    "    n_head,\n",
    "    n_hid,\n",
    "    n_layers,\n",
    "    dropout=dropout,\n",
    "    minibatch=minibatch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = None\n",
    "point_of_interest = None\n",
    "use_SR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Optimizer(model, Hamiltonians, point_of_interest=point_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "results_dir = \"results\"\n",
    "paper_checkpoint_name = \"ckpt_100000_Ising_32_8_8_0.ckpt\"\n",
    "paper_checkpoint_path = os.path.join(results_dir, paper_checkpoint_name)\n",
    "checkpoint = torch.load(paper_checkpoint_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.train(\n",
    "#     100,\n",
    "#     batch=1000000,\n",
    "#     max_unique=100,\n",
    "#     param_range=param_range,\n",
    "#     fine_tuning=False,\n",
    "#     use_SR=use_SR,\n",
    "#     ensemble_id=int(use_SR),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([8])\n"
     ]
    }
   ],
   "source": [
    "system_size = torch.tensor([8])\n",
    "param = torch.tensor([1])\n",
    "model.set_param(system_size=system_size, param=param)\n",
    "\n",
    "print(model.param)\n",
    "print(model.system_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(-0.0433, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# H = Hamiltonians[0]\n",
    "# batch = 100\n",
    "# max_unique = 10\n",
    "#\n",
    "# loss, log_amp, log_phase, sample_weight, Er, Ei, E_var = optim.minimize_energy_step(\n",
    "#     H, batch, max_unique, use_symmetry=True\n",
    "# )\n",
    "#\n",
    "# print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import compute_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log_amp: tensor([-2.1147, -3.5025, -4.4339, -3.9888, -4.5769, -5.6326, -5.1818, -4.2123,\n",
      "        -4.6144, -5.8951, -6.6344, -5.9671, -5.4398, -6.2230, -5.4674, -4.2806,\n",
      "        -4.6144, -5.9483, -6.8276, -6.2891, -6.8039, -7.7233, -7.1573, -6.0576,\n",
      "        -5.4992, -6.6536, -7.2473, -6.3880, -5.7522, -6.3660, -5.5076, -4.2123,\n",
      "        -4.5769, -5.9275, -6.8408, -6.3435, -6.9366, -7.9246, -7.4320, -6.3880,\n",
      "        -6.8039, -8.0068, -8.6798, -7.9045, -7.3736, -8.0424, -7.2328, -5.9671,\n",
      "        -5.4398, -6.7046, -7.5323, -6.8710, -7.3736, -8.1560, -7.4978, -6.2891,\n",
      "        -5.7522, -6.8057, -7.3242, -6.3435, -5.7287, -6.2404, -5.3412, -3.9888,\n",
      "        -4.4339, -5.7963, -6.7188, -6.2404, -6.8408, -7.8543, -7.3776, -6.3660,\n",
      "        -6.8276, -8.0660, -8.7722, -8.0424, -7.5323, -8.2475, -7.4618, -6.2230,\n",
      "        -6.6344, -7.9249, -8.7722, -8.1560, -8.6798, -9.5130, -8.8940, -7.7233,\n",
      "        -7.2473, -8.3323, -8.8733, -7.9246, -7.3242, -7.8543, -6.9746, -5.6326,\n",
      "        -5.1818, -6.4998, -7.3776, -6.8057, -7.4320, -8.3323, -7.7860, -6.6536,\n",
      "        -7.1573, -8.2917, -8.8940, -8.0068, -7.4978, -8.0660, -7.2202, -5.8951,\n",
      "        -5.4674, -6.6839, -7.4618, -6.7046, -7.2328, -7.9249, -7.2202, -5.9483,\n",
      "        -5.5076, -6.4998, -6.9746, -5.9275, -5.3412, -5.7963, -4.8867, -3.5025,\n",
      "        -3.5025, -4.8867, -5.7963, -5.3412, -5.9275, -6.9746, -6.4998, -5.5076,\n",
      "        -5.9483, -7.2202, -7.9249, -7.2328, -6.7046, -7.4618, -6.6839, -5.4674,\n",
      "        -5.8951, -7.2202, -8.0660, -7.4978, -8.0068, -8.8940, -8.2917, -7.1573,\n",
      "        -6.6536, -7.7860, -8.3323, -7.4320, -6.8057, -7.3776, -6.4998, -5.1818,\n",
      "        -5.6326, -6.9746, -7.8543, -7.3242, -7.9246, -8.8733, -8.3323, -7.2473,\n",
      "        -7.7233, -8.8940, -9.5130, -8.6798, -8.1560, -8.7722, -7.9249, -6.6344,\n",
      "        -6.2230, -7.4618, -8.2475, -7.5323, -8.0424, -8.7722, -8.0660, -6.8276,\n",
      "        -6.3660, -7.3776, -7.8543, -6.8408, -6.2404, -6.7188, -5.7963, -4.4339,\n",
      "        -3.9888, -5.3412, -6.2404, -5.7287, -6.3435, -7.3242, -6.8057, -5.7522,\n",
      "        -6.2891, -7.4978, -8.1560, -7.3736, -6.8710, -7.5323, -6.7046, -5.4398,\n",
      "        -5.9671, -7.2328, -8.0424, -7.3736, -7.9045, -8.6798, -8.0068, -6.8039,\n",
      "        -6.3880, -7.4320, -7.9246, -6.9366, -6.3435, -6.8408, -5.9275, -4.5769,\n",
      "        -4.2123, -5.5076, -6.3660, -5.7522, -6.3880, -7.2473, -6.6536, -5.4992,\n",
      "        -6.0576, -7.1573, -7.7233, -6.8039, -6.2891, -6.8276, -5.9483, -4.6144,\n",
      "        -4.2806, -5.4674, -6.2230, -5.4398, -5.9671, -6.6344, -5.8951, -4.6144,\n",
      "        -4.2123, -5.1818, -5.6326, -4.5769, -3.9888, -4.4339, -3.5025, -2.1147],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Log_phase: tensor([-0.1765,  6.1088,  6.1093, -0.1738,  6.1092, -0.1711, -0.1730,  6.1103,\n",
      "         6.1091, -0.1724, -0.1723,  6.1107, -0.1729,  6.1124,  6.1115, -0.1722,\n",
      "         6.1091, -0.1712, -0.1729,  6.1095, -0.1740,  6.1125,  6.1097, -0.1734,\n",
      "        -0.1734,  6.1099,  6.1110, -0.1731,  6.1085, -0.1711, -0.1717,  6.1103,\n",
      "         6.1092, -0.1706, -0.1719,  6.1107, -0.1743,  6.1129,  6.1086, -0.1731,\n",
      "        -0.1740,  6.1115,  6.1105, -0.1714,  6.1094, -0.1707, -0.1716,  6.1107,\n",
      "        -0.1729,  6.1118,  6.1094, -0.1748,  6.1094, -0.1715, -0.1728,  6.1095,\n",
      "         6.1085, -0.1738, -0.1710,  6.1107, -0.1743,  6.1123,  6.1121, -0.1738,\n",
      "         6.1093, -0.1696, -0.1709,  6.1123, -0.1719,  6.1150,  6.1112, -0.1711,\n",
      "        -0.1729,  6.1125,  6.1123, -0.1707,  6.1094, -0.1715, -0.1707,  6.1124,\n",
      "        -0.1723,  6.1137,  6.1123, -0.1715,  6.1105, -0.1705, -0.1712,  6.1125,\n",
      "         6.1110, -0.1712, -0.1694,  6.1129, -0.1710,  6.1150,  6.1149, -0.1711,\n",
      "        -0.1730,  6.1132,  6.1112, -0.1738,  6.1086, -0.1712, -0.1746,  6.1099,\n",
      "         6.1097, -0.1712, -0.1712,  6.1115, -0.1728,  6.1125,  6.1141, -0.1724,\n",
      "         6.1115, -0.1699, -0.1707,  6.1118, -0.1716,  6.1137,  6.1141, -0.1712,\n",
      "        -0.1717,  6.1132,  6.1149, -0.1706,  6.1121, -0.1696, -0.1695,  6.1088,\n",
      "         6.1088, -0.1695, -0.1696,  6.1121, -0.1706,  6.1149,  6.1132, -0.1717,\n",
      "        -0.1712,  6.1141,  6.1137, -0.1716,  6.1118, -0.1707, -0.1699,  6.1115,\n",
      "        -0.1724,  6.1141,  6.1125, -0.1728,  6.1115, -0.1712, -0.1712,  6.1097,\n",
      "         6.1099, -0.1746, -0.1712,  6.1086, -0.1738,  6.1112,  6.1132, -0.1730,\n",
      "        -0.1711,  6.1149,  6.1150, -0.1710,  6.1129, -0.1694, -0.1712,  6.1110,\n",
      "         6.1125, -0.1712, -0.1705,  6.1105, -0.1715,  6.1123,  6.1137, -0.1723,\n",
      "         6.1124, -0.1707, -0.1715,  6.1094, -0.1707,  6.1123,  6.1125, -0.1729,\n",
      "        -0.1711,  6.1112,  6.1150, -0.1719,  6.1123, -0.1709, -0.1696,  6.1093,\n",
      "        -0.1738,  6.1121,  6.1123, -0.1743,  6.1107, -0.1710, -0.1738,  6.1085,\n",
      "         6.1095, -0.1728, -0.1715,  6.1094, -0.1748,  6.1094,  6.1118, -0.1729,\n",
      "         6.1107, -0.1716, -0.1707,  6.1094, -0.1714,  6.1105,  6.1115, -0.1740,\n",
      "        -0.1731,  6.1086,  6.1129, -0.1743,  6.1107, -0.1719, -0.1706,  6.1092,\n",
      "         6.1103, -0.1717, -0.1711,  6.1085, -0.1731,  6.1110,  6.1099, -0.1734,\n",
      "        -0.1734,  6.1097,  6.1125, -0.1740,  6.1095, -0.1729, -0.1712,  6.1091,\n",
      "        -0.1722,  6.1115,  6.1124, -0.1729,  6.1107, -0.1723, -0.1724,  6.1091,\n",
      "         6.1103, -0.1730, -0.1711,  6.1092, -0.1738,  6.1093,  6.1088, -0.1765],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "basis = torch.tensor(H.generate_basis())\n",
    "symmetry = H.symmetry\n",
    "log_amp, log_phase = compute_psi(model, basis, symmetry, check_duplicate=True)\n",
    "print(\"Log_amp:\", log_amp)\n",
    "print(\"Log_phase:\", log_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amplitude: tensor([1.2067e-01, 3.0122e-02, 1.1868e-02, 1.8522e-02, 1.0286e-02, 3.5791e-03,\n",
      "        5.6181e-03, 1.4812e-02, 9.9086e-03, 2.7530e-03, 1.3143e-03, 2.5616e-03,\n",
      "        4.3402e-03, 1.9833e-03, 4.2224e-03, 1.3835e-02, 9.9086e-03, 2.6102e-03,\n",
      "        1.0835e-03, 1.8565e-03, 1.1094e-03, 4.4238e-04, 7.7913e-04, 2.3401e-03,\n",
      "        4.0899e-03, 1.2894e-03, 7.1212e-04, 1.6817e-03, 3.1757e-03, 1.7190e-03,\n",
      "        4.0559e-03, 1.4812e-02, 1.0286e-02, 2.6651e-03, 1.0692e-03, 1.7582e-03,\n",
      "        9.7161e-04, 3.6172e-04, 5.9201e-04, 1.6817e-03, 1.1094e-03, 3.3320e-04,\n",
      "        1.6998e-04, 3.6909e-04, 6.2759e-04, 3.2153e-04, 7.2251e-04, 2.5616e-03,\n",
      "        4.3402e-03, 1.2253e-03, 5.3553e-04, 1.0375e-03, 6.2759e-04, 2.8702e-04,\n",
      "        5.5432e-04, 1.8565e-03, 3.1757e-03, 1.1075e-03, 6.5939e-04, 1.7582e-03,\n",
      "        3.2514e-03, 1.9490e-03, 4.7902e-03, 1.8522e-02, 1.1868e-02, 3.0388e-03,\n",
      "        1.2080e-03, 1.9490e-03, 1.0692e-03, 3.8807e-04, 6.2511e-04, 1.7190e-03,\n",
      "        1.0835e-03, 3.1405e-04, 1.5498e-04, 3.2153e-04, 5.3553e-04, 2.6192e-04,\n",
      "        5.7463e-04, 1.9833e-03, 1.3143e-03, 3.6163e-04, 1.5498e-04, 2.8702e-04,\n",
      "        1.6998e-04, 7.3887e-05, 1.3721e-04, 4.4238e-04, 7.1212e-04, 2.4061e-04,\n",
      "        1.4007e-04, 3.6172e-04, 6.5939e-04, 3.8807e-04, 9.3533e-04, 3.5791e-03,\n",
      "        5.6181e-03, 1.5037e-03, 6.2511e-04, 1.1075e-03, 5.9201e-04, 2.4061e-04,\n",
      "        4.1549e-04, 1.2894e-03, 7.7913e-04, 2.5059e-04, 1.3721e-04, 3.3320e-04,\n",
      "        5.5432e-04, 3.1405e-04, 7.3169e-04, 2.7530e-03, 4.2224e-03, 1.2509e-03,\n",
      "        5.7463e-04, 1.2253e-03, 7.2251e-04, 3.6163e-04, 7.3169e-04, 2.6102e-03,\n",
      "        4.0559e-03, 1.5037e-03, 9.3533e-04, 2.6651e-03, 4.7902e-03, 3.0388e-03,\n",
      "        7.5461e-03, 3.0122e-02, 3.0122e-02, 7.5461e-03, 3.0388e-03, 4.7902e-03,\n",
      "        2.6651e-03, 9.3533e-04, 1.5037e-03, 4.0559e-03, 2.6102e-03, 7.3169e-04,\n",
      "        3.6163e-04, 7.2251e-04, 1.2253e-03, 5.7463e-04, 1.2509e-03, 4.2224e-03,\n",
      "        2.7530e-03, 7.3169e-04, 3.1405e-04, 5.5432e-04, 3.3320e-04, 1.3721e-04,\n",
      "        2.5059e-04, 7.7913e-04, 1.2894e-03, 4.1549e-04, 2.4061e-04, 5.9201e-04,\n",
      "        1.1075e-03, 6.2511e-04, 1.5037e-03, 5.6181e-03, 3.5791e-03, 9.3533e-04,\n",
      "        3.8807e-04, 6.5939e-04, 3.6172e-04, 1.4007e-04, 2.4061e-04, 7.1212e-04,\n",
      "        4.4238e-04, 1.3721e-04, 7.3887e-05, 1.6998e-04, 2.8702e-04, 1.5498e-04,\n",
      "        3.6163e-04, 1.3143e-03, 1.9833e-03, 5.7463e-04, 2.6192e-04, 5.3553e-04,\n",
      "        3.2153e-04, 1.5498e-04, 3.1405e-04, 1.0835e-03, 1.7190e-03, 6.2511e-04,\n",
      "        3.8807e-04, 1.0692e-03, 1.9490e-03, 1.2080e-03, 3.0388e-03, 1.1868e-02,\n",
      "        1.8522e-02, 4.7902e-03, 1.9490e-03, 3.2514e-03, 1.7582e-03, 6.5939e-04,\n",
      "        1.1075e-03, 3.1757e-03, 1.8565e-03, 5.5432e-04, 2.8702e-04, 6.2759e-04,\n",
      "        1.0375e-03, 5.3553e-04, 1.2253e-03, 4.3402e-03, 2.5616e-03, 7.2251e-04,\n",
      "        3.2153e-04, 6.2759e-04, 3.6909e-04, 1.6998e-04, 3.3320e-04, 1.1094e-03,\n",
      "        1.6817e-03, 5.9201e-04, 3.6172e-04, 9.7161e-04, 1.7582e-03, 1.0692e-03,\n",
      "        2.6651e-03, 1.0286e-02, 1.4812e-02, 4.0559e-03, 1.7190e-03, 3.1757e-03,\n",
      "        1.6817e-03, 7.1212e-04, 1.2894e-03, 4.0899e-03, 2.3401e-03, 7.7913e-04,\n",
      "        4.4238e-04, 1.1094e-03, 1.8565e-03, 1.0835e-03, 2.6102e-03, 9.9086e-03,\n",
      "        1.3835e-02, 4.2224e-03, 1.9833e-03, 4.3402e-03, 2.5616e-03, 1.3143e-03,\n",
      "        2.7530e-03, 9.9086e-03, 1.4812e-02, 5.6181e-03, 3.5791e-03, 1.0286e-02,\n",
      "        1.8522e-02, 1.1868e-02, 3.0122e-02, 1.2067e-01],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "Phase: tensor([  0.8382, 449.8196, 450.0327,   0.8404, 449.9704,   0.8427,   0.8411,\n",
      "        450.4782, 449.9447,   0.8417,   0.8417, 450.6434,   0.8412, 451.4286,\n",
      "        450.9983,   0.8418, 449.9447,   0.8427,   0.8412, 450.1305,   0.8403,\n",
      "        451.4452, 450.1904,   0.8408,   0.8408, 450.2902, 450.7775,   0.8410,\n",
      "        449.6687,   0.8427,   0.8422, 450.4782, 449.9704,   0.8432,   0.8421,\n",
      "        450.6354,   0.8400, 451.6364, 449.7276,   0.8410,   0.8403, 451.0122,\n",
      "        450.5744,   0.8425, 450.0610,   0.8431,   0.8423, 450.6434,   0.8412,\n",
      "        451.1613, 450.0646,   0.8396, 450.0610,   0.8424,   0.8413, 450.1305,\n",
      "        449.6687,   0.8405,   0.8428, 450.6354,   0.8401, 451.3677, 451.2769,\n",
      "          0.8404, 450.0327,   0.8440,   0.8429, 451.3677,   0.8421, 452.5970,\n",
      "        450.8639,   0.8427,   0.8412, 451.4564, 451.3748,   0.8431, 450.0646,\n",
      "          0.8424,   0.8431, 451.4286,   0.8417, 452.0013, 451.3748,   0.8424,\n",
      "        450.5744,   0.8433,   0.8426, 451.4452, 450.7775,   0.8427,   0.8442,\n",
      "        451.6364,   0.8428, 452.5970, 452.5301,   0.8427,   0.8411, 451.7910,\n",
      "        450.8639,   0.8405, 449.7276,   0.8427,   0.8398, 450.2902, 450.1904,\n",
      "          0.8426,   0.8426, 451.0122,   0.8413, 451.4564, 452.1740,   0.8417,\n",
      "        450.9983,   0.8437,   0.8431, 451.1613,   0.8423, 452.0013, 452.1740,\n",
      "          0.8427,   0.8422, 451.7910, 452.5301,   0.8432, 451.2769,   0.8440,\n",
      "          0.8441, 449.8196, 449.8196,   0.8441,   0.8440, 451.2769,   0.8432,\n",
      "        452.5301, 451.7910,   0.8422,   0.8427, 452.1740, 452.0013,   0.8423,\n",
      "        451.1613,   0.8431,   0.8437, 450.9983,   0.8417, 452.1740, 451.4564,\n",
      "          0.8413, 451.0122,   0.8426,   0.8426, 450.1904, 450.2902,   0.8398,\n",
      "          0.8427, 449.7276,   0.8405, 450.8639, 451.7910,   0.8411,   0.8427,\n",
      "        452.5301, 452.5970,   0.8428, 451.6364,   0.8442,   0.8427, 450.7775,\n",
      "        451.4452,   0.8426,   0.8433, 450.5744,   0.8424, 451.3748, 452.0013,\n",
      "          0.8417, 451.4286,   0.8431,   0.8424, 450.0646,   0.8431, 451.3748,\n",
      "        451.4564,   0.8412,   0.8427, 450.8639, 452.5970,   0.8421, 451.3677,\n",
      "          0.8429,   0.8440, 450.0327,   0.8404, 451.2769, 451.3677,   0.8401,\n",
      "        450.6354,   0.8428,   0.8405, 449.6687, 450.1305,   0.8413,   0.8424,\n",
      "        450.0610,   0.8396, 450.0646, 451.1613,   0.8412, 450.6434,   0.8423,\n",
      "          0.8431, 450.0610,   0.8425, 450.5744, 451.0122,   0.8403,   0.8410,\n",
      "        449.7276, 451.6364,   0.8400, 450.6354,   0.8421,   0.8432, 449.9704,\n",
      "        450.4782,   0.8422,   0.8427, 449.6687,   0.8410, 450.7775, 450.2902,\n",
      "          0.8408,   0.8408, 450.1904, 451.4452,   0.8403, 450.1305,   0.8412,\n",
      "          0.8427, 449.9447,   0.8418, 450.9983, 451.4286,   0.8412, 450.6434,\n",
      "          0.8417,   0.8417, 449.9447, 450.4782,   0.8411,   0.8427, 449.9704,\n",
      "          0.8404, 450.0327, 449.8196,   0.8382], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "amp = torch.exp(log_amp)\n",
    "phase = torch.exp(log_phase)\n",
    "\n",
    "print(\"Amplitude:\", amp)\n",
    "print(\"Phase:\", phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1880e-01-2.1194e-02j, 2.9666e-02-5.2249e-03j, 1.1689e-02-2.0531e-03j,\n",
      "        1.8243e-02-3.2033e-03j, 1.0131e-02-1.7808e-03j, 3.5269e-03-6.0937e-04j,\n",
      "        5.5343e-03-9.6709e-04j, 1.4591e-02-2.5479e-03j, 9.7589e-03-1.7160e-03j,\n",
      "        2.7122e-03-4.7215e-04j, 1.2949e-03-2.2536e-04j, 2.5236e-03-4.3972e-04j,\n",
      "        4.2755e-03-7.4673e-04j, 1.9544e-03-3.3704e-04j, 4.1603e-03-7.2152e-04j,\n",
      "        1.3630e-02-2.3707e-03j, 9.7589e-03-1.7160e-03j, 2.5721e-03-4.4465e-04j,\n",
      "        1.0673e-03-1.8645e-04j, 1.8286e-03-3.2076e-04j, 1.0926e-03-1.9202e-04j,\n",
      "        4.3595e-04-7.5161e-05j, 7.6743e-04-1.3451e-04j, 2.3050e-03-4.0369e-04j,\n",
      "        4.0286e-03-7.0560e-04j, 1.2701e-03-2.2233e-04j, 7.0158e-04-1.2203e-04j,\n",
      "        1.6565e-03-2.8970e-04j, 3.1274e-03-5.5190e-04j, 1.6939e-03-2.9270e-04j,\n",
      "        3.9962e-03-6.9318e-04j, 1.4591e-02-2.5479e-03j, 1.0131e-02-1.7808e-03j,\n",
      "        2.6264e-03-4.5244e-04j, 1.0535e-03-1.8284e-04j, 1.7321e-03-3.0183e-04j,\n",
      "        9.5689e-04-1.6853e-04j, 3.5649e-04-6.1306e-05j, 5.8302e-04-1.0281e-04j,\n",
      "        1.6565e-03-2.8970e-04j, 1.0926e-03-1.9202e-04j, 3.2830e-04-5.6926e-05j,\n",
      "        1.6746e-04-2.9204e-05j, 3.6368e-04-6.2953e-05j, 6.1814e-04-1.0853e-04j,\n",
      "        3.1686e-04-5.4626e-05j, 7.1190e-04-1.2336e-04j, 2.5236e-03-4.3972e-04j,\n",
      "        4.2755e-03-7.4673e-04j, 1.2074e-03-2.0894e-04j, 5.2746e-04-9.2604e-05j,\n",
      "        1.0217e-03-1.8041e-04j, 6.1814e-04-1.0853e-04j, 2.8281e-04-4.8988e-05j,\n",
      "        5.4606e-04-9.5332e-05j, 1.8286e-03-3.2076e-04j, 3.1274e-03-5.5190e-04j,\n",
      "        1.0908e-03-1.9151e-04j, 6.4977e-04-1.1222e-04j, 1.7321e-03-3.0183e-04j,\n",
      "        3.2021e-03-5.6370e-04j, 1.9206e-03-3.3148e-04j, 4.7203e-03-8.1564e-04j,\n",
      "        1.8243e-02-3.2033e-03j, 1.1689e-02-2.0531e-03j, 2.9952e-03-5.1306e-04j,\n",
      "        1.1904e-03-2.0542e-04j, 1.9206e-03-3.3148e-04j, 1.0535e-03-1.8284e-04j,\n",
      "        3.8259e-04-6.4959e-05j, 6.1588e-04-1.0700e-04j, 1.6939e-03-2.9270e-04j,\n",
      "        1.0673e-03-1.8645e-04j, 3.0948e-04-5.3350e-05j, 1.5273e-04-2.6356e-05j,\n",
      "        3.1686e-04-5.4626e-05j, 5.2746e-04-9.2604e-05j, 2.5808e-04-4.4698e-05j,\n",
      "        5.6628e-04-9.7609e-05j, 1.9544e-03-3.3704e-04j, 1.2949e-03-2.2536e-04j,\n",
      "        3.5644e-04-6.1003e-05j, 1.5273e-04-2.6356e-05j, 2.8281e-04-4.8988e-05j,\n",
      "        1.6746e-04-2.9204e-05j, 7.2816e-05-1.2535e-05j, 1.3520e-04-2.3380e-05j,\n",
      "        4.3595e-04-7.5161e-05j, 7.0158e-04-1.2203e-04j, 2.3709e-04-4.0981e-05j,\n",
      "        1.3807e-04-2.3612e-05j, 3.5649e-04-6.1306e-05j, 6.4977e-04-1.1222e-04j,\n",
      "        3.8259e-04-6.4959e-05j, 9.2211e-04-1.5670e-04j, 3.5269e-03-6.0937e-04j,\n",
      "        5.5343e-03-9.6709e-04j, 1.4820e-03-2.5434e-04j, 6.1588e-04-1.0700e-04j,\n",
      "        1.0908e-03-1.9151e-04j, 5.8302e-04-1.0281e-04j, 2.3709e-04-4.0981e-05j,\n",
      "        4.0918e-04-7.2165e-05j, 1.2701e-03-2.2233e-04j, 7.6743e-04-1.3451e-04j,\n",
      "        2.4693e-04-4.2697e-05j, 1.3520e-04-2.3380e-05j, 3.2830e-04-5.6926e-05j,\n",
      "        5.4606e-04-9.5332e-05j, 3.0948e-04-5.3350e-05j, 7.2125e-04-1.2315e-04j,\n",
      "        2.7122e-03-4.7215e-04j, 4.1603e-03-7.2152e-04j, 1.2328e-03-2.1153e-04j,\n",
      "        5.6628e-04-9.7609e-05j, 1.2074e-03-2.0894e-04j, 7.1190e-04-1.2336e-04j,\n",
      "        3.5644e-04-6.1003e-05j, 7.2125e-04-1.2315e-04j, 2.5721e-03-4.4465e-04j,\n",
      "        3.9962e-03-6.9318e-04j, 1.4820e-03-2.5434e-04j, 9.2211e-04-1.5670e-04j,\n",
      "        2.6264e-03-4.5244e-04j, 4.7203e-03-8.1564e-04j, 2.9952e-03-5.1306e-04j,\n",
      "        7.4379e-03-1.2732e-03j, 2.9666e-02-5.2249e-03j, 2.9666e-02-5.2249e-03j,\n",
      "        7.4379e-03-1.2732e-03j, 2.9952e-03-5.1306e-04j, 4.7203e-03-8.1564e-04j,\n",
      "        2.6264e-03-4.5244e-04j, 9.2211e-04-1.5670e-04j, 1.4820e-03-2.5434e-04j,\n",
      "        3.9962e-03-6.9318e-04j, 2.5721e-03-4.4465e-04j, 7.2125e-04-1.2315e-04j,\n",
      "        3.5644e-04-6.1003e-05j, 7.1190e-04-1.2336e-04j, 1.2074e-03-2.0894e-04j,\n",
      "        5.6628e-04-9.7609e-05j, 1.2328e-03-2.1153e-04j, 4.1603e-03-7.2152e-04j,\n",
      "        2.7122e-03-4.7215e-04j, 7.2125e-04-1.2315e-04j, 3.0948e-04-5.3350e-05j,\n",
      "        5.4606e-04-9.5332e-05j, 3.2830e-04-5.6926e-05j, 1.3520e-04-2.3380e-05j,\n",
      "        2.4693e-04-4.2697e-05j, 7.6743e-04-1.3451e-04j, 1.2701e-03-2.2233e-04j,\n",
      "        4.0918e-04-7.2165e-05j, 2.3709e-04-4.0981e-05j, 5.8302e-04-1.0281e-04j,\n",
      "        1.0908e-03-1.9151e-04j, 6.1588e-04-1.0700e-04j, 1.4820e-03-2.5434e-04j,\n",
      "        5.5343e-03-9.6709e-04j, 3.5269e-03-6.0937e-04j, 9.2211e-04-1.5670e-04j,\n",
      "        3.8259e-04-6.4959e-05j, 6.4977e-04-1.1222e-04j, 3.5649e-04-6.1306e-05j,\n",
      "        1.3807e-04-2.3612e-05j, 2.3709e-04-4.0981e-05j, 7.0158e-04-1.2203e-04j,\n",
      "        4.3595e-04-7.5161e-05j, 1.3520e-04-2.3380e-05j, 7.2816e-05-1.2535e-05j,\n",
      "        1.6746e-04-2.9204e-05j, 2.8281e-04-4.8988e-05j, 1.5273e-04-2.6356e-05j,\n",
      "        3.5644e-04-6.1003e-05j, 1.2949e-03-2.2536e-04j, 1.9544e-03-3.3704e-04j,\n",
      "        5.6628e-04-9.7609e-05j, 2.5808e-04-4.4698e-05j, 5.2746e-04-9.2604e-05j,\n",
      "        3.1686e-04-5.4626e-05j, 1.5273e-04-2.6356e-05j, 3.0948e-04-5.3350e-05j,\n",
      "        1.0673e-03-1.8645e-04j, 1.6939e-03-2.9270e-04j, 6.1588e-04-1.0700e-04j,\n",
      "        3.8259e-04-6.4959e-05j, 1.0535e-03-1.8284e-04j, 1.9206e-03-3.3148e-04j,\n",
      "        1.1904e-03-2.0542e-04j, 2.9952e-03-5.1306e-04j, 1.1689e-02-2.0531e-03j,\n",
      "        1.8243e-02-3.2033e-03j, 4.7203e-03-8.1564e-04j, 1.9206e-03-3.3148e-04j,\n",
      "        3.2021e-03-5.6370e-04j, 1.7321e-03-3.0183e-04j, 6.4977e-04-1.1222e-04j,\n",
      "        1.0908e-03-1.9151e-04j, 3.1274e-03-5.5190e-04j, 1.8286e-03-3.2076e-04j,\n",
      "        5.4606e-04-9.5332e-05j, 2.8281e-04-4.8988e-05j, 6.1814e-04-1.0853e-04j,\n",
      "        1.0217e-03-1.8041e-04j, 5.2746e-04-9.2604e-05j, 1.2074e-03-2.0894e-04j,\n",
      "        4.2755e-03-7.4673e-04j, 2.5236e-03-4.3972e-04j, 7.1190e-04-1.2336e-04j,\n",
      "        3.1686e-04-5.4626e-05j, 6.1814e-04-1.0853e-04j, 3.6368e-04-6.2953e-05j,\n",
      "        1.6746e-04-2.9204e-05j, 3.2830e-04-5.6926e-05j, 1.0926e-03-1.9202e-04j,\n",
      "        1.6565e-03-2.8970e-04j, 5.8302e-04-1.0281e-04j, 3.5649e-04-6.1306e-05j,\n",
      "        9.5689e-04-1.6853e-04j, 1.7321e-03-3.0183e-04j, 1.0535e-03-1.8284e-04j,\n",
      "        2.6264e-03-4.5244e-04j, 1.0131e-02-1.7808e-03j, 1.4591e-02-2.5479e-03j,\n",
      "        3.9962e-03-6.9318e-04j, 1.6939e-03-2.9270e-04j, 3.1274e-03-5.5190e-04j,\n",
      "        1.6565e-03-2.8970e-04j, 7.0158e-04-1.2203e-04j, 1.2701e-03-2.2233e-04j,\n",
      "        4.0286e-03-7.0560e-04j, 2.3050e-03-4.0369e-04j, 7.6743e-04-1.3451e-04j,\n",
      "        4.3595e-04-7.5161e-05j, 1.0926e-03-1.9202e-04j, 1.8286e-03-3.2076e-04j,\n",
      "        1.0673e-03-1.8645e-04j, 2.5721e-03-4.4465e-04j, 9.7589e-03-1.7160e-03j,\n",
      "        1.3630e-02-2.3707e-03j, 4.1603e-03-7.2152e-04j, 1.9544e-03-3.3704e-04j,\n",
      "        4.2755e-03-7.4673e-04j, 2.5236e-03-4.3972e-04j, 1.2949e-03-2.2536e-04j,\n",
      "        2.7122e-03-4.7215e-04j, 9.7589e-03-1.7160e-03j, 1.4591e-02-2.5479e-03j,\n",
      "        5.5343e-03-9.6709e-04j, 3.5269e-03-6.0937e-04j, 1.0131e-02-1.7808e-03j,\n",
      "        1.8243e-02-3.2033e-03j, 1.1689e-02-2.0531e-03j, 2.9666e-02-5.2249e-03j,\n",
      "        1.1880e-01-2.1194e-02j], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "psi_predicted = torch.exp(log_amp) * torch.exp(1j * log_phase)\n",
    "print(psi_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4591, -0.1292, -0.1292,  0.0774, -0.1292,  0.0404,  0.0774, -0.0607,\n",
      "        -0.1292,  0.0378,  0.0404, -0.0274,  0.0774, -0.0274, -0.0607,  0.0564,\n",
      "        -0.1292,  0.0375,  0.0378, -0.0246,  0.0404, -0.0137, -0.0274,  0.0247,\n",
      "         0.0774, -0.0246, -0.0274,  0.0221, -0.0607,  0.0247,  0.0564, -0.0607,\n",
      "        -0.1292,  0.0378,  0.0375, -0.0246,  0.0378, -0.0128, -0.0246,  0.0221,\n",
      "         0.0404, -0.0128, -0.0137,  0.0110, -0.0274,  0.0113,  0.0247, -0.0274,\n",
      "         0.0774, -0.0246, -0.0246,  0.0192, -0.0274,  0.0110,  0.0221, -0.0246,\n",
      "        -0.0607,  0.0221,  0.0247, -0.0246,  0.0564, -0.0274, -0.0607,  0.0774,\n",
      "        -0.1292,  0.0404,  0.0378, -0.0274,  0.0375, -0.0137, -0.0246,  0.0247,\n",
      "         0.0378, -0.0128, -0.0128,  0.0113, -0.0246,  0.0110,  0.0221, -0.0274,\n",
      "         0.0404, -0.0137, -0.0128,  0.0110, -0.0137,  0.0060,  0.0110, -0.0137,\n",
      "        -0.0274,  0.0110,  0.0113, -0.0128,  0.0247, -0.0137, -0.0274,  0.0404,\n",
      "         0.0774, -0.0274, -0.0246,  0.0221, -0.0246,  0.0110,  0.0192, -0.0246,\n",
      "        -0.0274,  0.0113,  0.0110, -0.0128,  0.0221, -0.0128, -0.0246,  0.0378,\n",
      "        -0.0607,  0.0247,  0.0221, -0.0246,  0.0247, -0.0137, -0.0246,  0.0375,\n",
      "         0.0564, -0.0274, -0.0274,  0.0378, -0.0607,  0.0404,  0.0774, -0.1292,\n",
      "        -0.1292,  0.0774,  0.0404, -0.0607,  0.0378, -0.0274, -0.0274,  0.0564,\n",
      "         0.0375, -0.0246, -0.0137,  0.0247, -0.0246,  0.0221,  0.0247, -0.0607,\n",
      "         0.0378, -0.0246, -0.0128,  0.0221, -0.0128,  0.0110,  0.0113, -0.0274,\n",
      "        -0.0246,  0.0192,  0.0110, -0.0246,  0.0221, -0.0246, -0.0274,  0.0774,\n",
      "         0.0404, -0.0274, -0.0137,  0.0247, -0.0128,  0.0113,  0.0110, -0.0274,\n",
      "        -0.0137,  0.0110,  0.0060, -0.0137,  0.0110, -0.0128, -0.0137,  0.0404,\n",
      "        -0.0274,  0.0221,  0.0110, -0.0246,  0.0113, -0.0128, -0.0128,  0.0378,\n",
      "         0.0247, -0.0246, -0.0137,  0.0375, -0.0274,  0.0378,  0.0404, -0.1292,\n",
      "         0.0774, -0.0607, -0.0274,  0.0564, -0.0246,  0.0247,  0.0221, -0.0607,\n",
      "        -0.0246,  0.0221,  0.0110, -0.0274,  0.0192, -0.0246, -0.0246,  0.0774,\n",
      "        -0.0274,  0.0247,  0.0113, -0.0274,  0.0110, -0.0137, -0.0128,  0.0404,\n",
      "         0.0221, -0.0246, -0.0128,  0.0378, -0.0246,  0.0375,  0.0378, -0.1292,\n",
      "        -0.0607,  0.0564,  0.0247, -0.0607,  0.0221, -0.0274, -0.0246,  0.0774,\n",
      "         0.0247, -0.0274, -0.0137,  0.0404, -0.0246,  0.0378,  0.0375, -0.1292,\n",
      "         0.0564, -0.0607, -0.0274,  0.0774, -0.0274,  0.0404,  0.0378, -0.1292,\n",
      "        -0.0607,  0.0774,  0.0404, -0.1292,  0.0774, -0.1292, -0.1292,  0.4591],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "energy, psi_true = H.calc_ground(param=1)\n",
    "psi_true = torch.tensor(psi_true)\n",
    "print(psi_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(0.0034+6.9522e-05j, dtype=torch.complex128, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse = torch.mean((psi_predicted - psi_true) ** 2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1880e-01, -2.1194e-02],\n",
      "        [ 2.9666e-02, -5.2249e-03],\n",
      "        [ 1.1689e-02, -2.0531e-03],\n",
      "        [ 1.8243e-02, -3.2033e-03],\n",
      "        [ 1.0131e-02, -1.7808e-03],\n",
      "        [ 3.5269e-03, -6.0937e-04],\n",
      "        [ 5.5343e-03, -9.6709e-04],\n",
      "        [ 1.4591e-02, -2.5479e-03],\n",
      "        [ 9.7589e-03, -1.7160e-03],\n",
      "        [ 2.7122e-03, -4.7215e-04],\n",
      "        [ 1.2949e-03, -2.2536e-04],\n",
      "        [ 2.5236e-03, -4.3972e-04],\n",
      "        [ 4.2755e-03, -7.4673e-04],\n",
      "        [ 1.9544e-03, -3.3704e-04],\n",
      "        [ 4.1603e-03, -7.2152e-04],\n",
      "        [ 1.3630e-02, -2.3707e-03],\n",
      "        [ 9.7589e-03, -1.7160e-03],\n",
      "        [ 2.5721e-03, -4.4465e-04],\n",
      "        [ 1.0673e-03, -1.8645e-04],\n",
      "        [ 1.8286e-03, -3.2076e-04],\n",
      "        [ 1.0926e-03, -1.9202e-04],\n",
      "        [ 4.3595e-04, -7.5161e-05],\n",
      "        [ 7.6743e-04, -1.3451e-04],\n",
      "        [ 2.3050e-03, -4.0369e-04],\n",
      "        [ 4.0286e-03, -7.0560e-04],\n",
      "        [ 1.2701e-03, -2.2233e-04],\n",
      "        [ 7.0158e-04, -1.2203e-04],\n",
      "        [ 1.6565e-03, -2.8970e-04],\n",
      "        [ 3.1274e-03, -5.5190e-04],\n",
      "        [ 1.6939e-03, -2.9270e-04],\n",
      "        [ 3.9962e-03, -6.9318e-04],\n",
      "        [ 1.4591e-02, -2.5479e-03],\n",
      "        [ 1.0131e-02, -1.7808e-03],\n",
      "        [ 2.6264e-03, -4.5244e-04],\n",
      "        [ 1.0535e-03, -1.8284e-04],\n",
      "        [ 1.7321e-03, -3.0183e-04],\n",
      "        [ 9.5689e-04, -1.6853e-04],\n",
      "        [ 3.5649e-04, -6.1306e-05],\n",
      "        [ 5.8302e-04, -1.0281e-04],\n",
      "        [ 1.6565e-03, -2.8970e-04],\n",
      "        [ 1.0926e-03, -1.9202e-04],\n",
      "        [ 3.2830e-04, -5.6926e-05],\n",
      "        [ 1.6746e-04, -2.9204e-05],\n",
      "        [ 3.6368e-04, -6.2953e-05],\n",
      "        [ 6.1814e-04, -1.0853e-04],\n",
      "        [ 3.1686e-04, -5.4626e-05],\n",
      "        [ 7.1190e-04, -1.2336e-04],\n",
      "        [ 2.5236e-03, -4.3972e-04],\n",
      "        [ 4.2755e-03, -7.4673e-04],\n",
      "        [ 1.2074e-03, -2.0894e-04],\n",
      "        [ 5.2746e-04, -9.2604e-05],\n",
      "        [ 1.0217e-03, -1.8041e-04],\n",
      "        [ 6.1814e-04, -1.0853e-04],\n",
      "        [ 2.8281e-04, -4.8988e-05],\n",
      "        [ 5.4606e-04, -9.5332e-05],\n",
      "        [ 1.8286e-03, -3.2076e-04],\n",
      "        [ 3.1274e-03, -5.5190e-04],\n",
      "        [ 1.0908e-03, -1.9151e-04],\n",
      "        [ 6.4977e-04, -1.1222e-04],\n",
      "        [ 1.7321e-03, -3.0183e-04],\n",
      "        [ 3.2021e-03, -5.6370e-04],\n",
      "        [ 1.9206e-03, -3.3148e-04],\n",
      "        [ 4.7203e-03, -8.1564e-04],\n",
      "        [ 1.8243e-02, -3.2033e-03],\n",
      "        [ 1.1689e-02, -2.0531e-03],\n",
      "        [ 2.9952e-03, -5.1306e-04],\n",
      "        [ 1.1904e-03, -2.0542e-04],\n",
      "        [ 1.9206e-03, -3.3148e-04],\n",
      "        [ 1.0535e-03, -1.8284e-04],\n",
      "        [ 3.8259e-04, -6.4959e-05],\n",
      "        [ 6.1588e-04, -1.0700e-04],\n",
      "        [ 1.6939e-03, -2.9270e-04],\n",
      "        [ 1.0673e-03, -1.8645e-04],\n",
      "        [ 3.0948e-04, -5.3350e-05],\n",
      "        [ 1.5273e-04, -2.6356e-05],\n",
      "        [ 3.1686e-04, -5.4626e-05],\n",
      "        [ 5.2746e-04, -9.2604e-05],\n",
      "        [ 2.5808e-04, -4.4698e-05],\n",
      "        [ 5.6628e-04, -9.7609e-05],\n",
      "        [ 1.9544e-03, -3.3704e-04],\n",
      "        [ 1.2949e-03, -2.2536e-04],\n",
      "        [ 3.5644e-04, -6.1003e-05],\n",
      "        [ 1.5273e-04, -2.6356e-05],\n",
      "        [ 2.8281e-04, -4.8988e-05],\n",
      "        [ 1.6746e-04, -2.9204e-05],\n",
      "        [ 7.2816e-05, -1.2535e-05],\n",
      "        [ 1.3520e-04, -2.3380e-05],\n",
      "        [ 4.3595e-04, -7.5161e-05],\n",
      "        [ 7.0158e-04, -1.2203e-04],\n",
      "        [ 2.3709e-04, -4.0981e-05],\n",
      "        [ 1.3807e-04, -2.3612e-05],\n",
      "        [ 3.5649e-04, -6.1306e-05],\n",
      "        [ 6.4977e-04, -1.1222e-04],\n",
      "        [ 3.8259e-04, -6.4959e-05],\n",
      "        [ 9.2211e-04, -1.5670e-04],\n",
      "        [ 3.5269e-03, -6.0937e-04],\n",
      "        [ 5.5343e-03, -9.6709e-04],\n",
      "        [ 1.4820e-03, -2.5434e-04],\n",
      "        [ 6.1588e-04, -1.0700e-04],\n",
      "        [ 1.0908e-03, -1.9151e-04],\n",
      "        [ 5.8302e-04, -1.0281e-04],\n",
      "        [ 2.3709e-04, -4.0981e-05],\n",
      "        [ 4.0918e-04, -7.2165e-05],\n",
      "        [ 1.2701e-03, -2.2233e-04],\n",
      "        [ 7.6743e-04, -1.3451e-04],\n",
      "        [ 2.4693e-04, -4.2697e-05],\n",
      "        [ 1.3520e-04, -2.3380e-05],\n",
      "        [ 3.2830e-04, -5.6926e-05],\n",
      "        [ 5.4606e-04, -9.5332e-05],\n",
      "        [ 3.0948e-04, -5.3350e-05],\n",
      "        [ 7.2125e-04, -1.2315e-04],\n",
      "        [ 2.7122e-03, -4.7215e-04],\n",
      "        [ 4.1603e-03, -7.2152e-04],\n",
      "        [ 1.2328e-03, -2.1153e-04],\n",
      "        [ 5.6628e-04, -9.7609e-05],\n",
      "        [ 1.2074e-03, -2.0894e-04],\n",
      "        [ 7.1190e-04, -1.2336e-04],\n",
      "        [ 3.5644e-04, -6.1003e-05],\n",
      "        [ 7.2125e-04, -1.2315e-04],\n",
      "        [ 2.5721e-03, -4.4465e-04],\n",
      "        [ 3.9962e-03, -6.9318e-04],\n",
      "        [ 1.4820e-03, -2.5434e-04],\n",
      "        [ 9.2211e-04, -1.5670e-04],\n",
      "        [ 2.6264e-03, -4.5244e-04],\n",
      "        [ 4.7203e-03, -8.1564e-04],\n",
      "        [ 2.9952e-03, -5.1306e-04],\n",
      "        [ 7.4379e-03, -1.2732e-03],\n",
      "        [ 2.9666e-02, -5.2249e-03],\n",
      "        [ 2.9666e-02, -5.2249e-03],\n",
      "        [ 7.4379e-03, -1.2732e-03],\n",
      "        [ 2.9952e-03, -5.1306e-04],\n",
      "        [ 4.7203e-03, -8.1564e-04],\n",
      "        [ 2.6264e-03, -4.5244e-04],\n",
      "        [ 9.2211e-04, -1.5670e-04],\n",
      "        [ 1.4820e-03, -2.5434e-04],\n",
      "        [ 3.9962e-03, -6.9318e-04],\n",
      "        [ 2.5721e-03, -4.4465e-04],\n",
      "        [ 7.2125e-04, -1.2315e-04],\n",
      "        [ 3.5644e-04, -6.1003e-05],\n",
      "        [ 7.1190e-04, -1.2336e-04],\n",
      "        [ 1.2074e-03, -2.0894e-04],\n",
      "        [ 5.6628e-04, -9.7609e-05],\n",
      "        [ 1.2328e-03, -2.1153e-04],\n",
      "        [ 4.1603e-03, -7.2152e-04],\n",
      "        [ 2.7122e-03, -4.7215e-04],\n",
      "        [ 7.2125e-04, -1.2315e-04],\n",
      "        [ 3.0948e-04, -5.3350e-05],\n",
      "        [ 5.4606e-04, -9.5332e-05],\n",
      "        [ 3.2830e-04, -5.6926e-05],\n",
      "        [ 1.3520e-04, -2.3380e-05],\n",
      "        [ 2.4693e-04, -4.2697e-05],\n",
      "        [ 7.6743e-04, -1.3451e-04],\n",
      "        [ 1.2701e-03, -2.2233e-04],\n",
      "        [ 4.0918e-04, -7.2165e-05],\n",
      "        [ 2.3709e-04, -4.0981e-05],\n",
      "        [ 5.8302e-04, -1.0281e-04],\n",
      "        [ 1.0908e-03, -1.9151e-04],\n",
      "        [ 6.1588e-04, -1.0700e-04],\n",
      "        [ 1.4820e-03, -2.5434e-04],\n",
      "        [ 5.5343e-03, -9.6709e-04],\n",
      "        [ 3.5269e-03, -6.0937e-04],\n",
      "        [ 9.2211e-04, -1.5670e-04],\n",
      "        [ 3.8259e-04, -6.4959e-05],\n",
      "        [ 6.4977e-04, -1.1222e-04],\n",
      "        [ 3.5649e-04, -6.1306e-05],\n",
      "        [ 1.3807e-04, -2.3612e-05],\n",
      "        [ 2.3709e-04, -4.0981e-05],\n",
      "        [ 7.0158e-04, -1.2203e-04],\n",
      "        [ 4.3595e-04, -7.5161e-05],\n",
      "        [ 1.3520e-04, -2.3380e-05],\n",
      "        [ 7.2816e-05, -1.2535e-05],\n",
      "        [ 1.6746e-04, -2.9204e-05],\n",
      "        [ 2.8281e-04, -4.8988e-05],\n",
      "        [ 1.5273e-04, -2.6356e-05],\n",
      "        [ 3.5644e-04, -6.1003e-05],\n",
      "        [ 1.2949e-03, -2.2536e-04],\n",
      "        [ 1.9544e-03, -3.3704e-04],\n",
      "        [ 5.6628e-04, -9.7609e-05],\n",
      "        [ 2.5808e-04, -4.4698e-05],\n",
      "        [ 5.2746e-04, -9.2604e-05],\n",
      "        [ 3.1686e-04, -5.4626e-05],\n",
      "        [ 1.5273e-04, -2.6356e-05],\n",
      "        [ 3.0948e-04, -5.3350e-05],\n",
      "        [ 1.0673e-03, -1.8645e-04],\n",
      "        [ 1.6939e-03, -2.9270e-04],\n",
      "        [ 6.1588e-04, -1.0700e-04],\n",
      "        [ 3.8259e-04, -6.4959e-05],\n",
      "        [ 1.0535e-03, -1.8284e-04],\n",
      "        [ 1.9206e-03, -3.3148e-04],\n",
      "        [ 1.1904e-03, -2.0542e-04],\n",
      "        [ 2.9952e-03, -5.1306e-04],\n",
      "        [ 1.1689e-02, -2.0531e-03],\n",
      "        [ 1.8243e-02, -3.2033e-03],\n",
      "        [ 4.7203e-03, -8.1564e-04],\n",
      "        [ 1.9206e-03, -3.3148e-04],\n",
      "        [ 3.2021e-03, -5.6370e-04],\n",
      "        [ 1.7321e-03, -3.0183e-04],\n",
      "        [ 6.4977e-04, -1.1222e-04],\n",
      "        [ 1.0908e-03, -1.9151e-04],\n",
      "        [ 3.1274e-03, -5.5190e-04],\n",
      "        [ 1.8286e-03, -3.2076e-04],\n",
      "        [ 5.4606e-04, -9.5332e-05],\n",
      "        [ 2.8281e-04, -4.8988e-05],\n",
      "        [ 6.1814e-04, -1.0853e-04],\n",
      "        [ 1.0217e-03, -1.8041e-04],\n",
      "        [ 5.2746e-04, -9.2604e-05],\n",
      "        [ 1.2074e-03, -2.0894e-04],\n",
      "        [ 4.2755e-03, -7.4673e-04],\n",
      "        [ 2.5236e-03, -4.3972e-04],\n",
      "        [ 7.1190e-04, -1.2336e-04],\n",
      "        [ 3.1686e-04, -5.4626e-05],\n",
      "        [ 6.1814e-04, -1.0853e-04],\n",
      "        [ 3.6368e-04, -6.2953e-05],\n",
      "        [ 1.6746e-04, -2.9204e-05],\n",
      "        [ 3.2830e-04, -5.6926e-05],\n",
      "        [ 1.0926e-03, -1.9202e-04],\n",
      "        [ 1.6565e-03, -2.8970e-04],\n",
      "        [ 5.8302e-04, -1.0281e-04],\n",
      "        [ 3.5649e-04, -6.1306e-05],\n",
      "        [ 9.5689e-04, -1.6853e-04],\n",
      "        [ 1.7321e-03, -3.0183e-04],\n",
      "        [ 1.0535e-03, -1.8284e-04],\n",
      "        [ 2.6264e-03, -4.5244e-04],\n",
      "        [ 1.0131e-02, -1.7808e-03],\n",
      "        [ 1.4591e-02, -2.5479e-03],\n",
      "        [ 3.9962e-03, -6.9318e-04],\n",
      "        [ 1.6939e-03, -2.9270e-04],\n",
      "        [ 3.1274e-03, -5.5190e-04],\n",
      "        [ 1.6565e-03, -2.8970e-04],\n",
      "        [ 7.0158e-04, -1.2203e-04],\n",
      "        [ 1.2701e-03, -2.2233e-04],\n",
      "        [ 4.0286e-03, -7.0560e-04],\n",
      "        [ 2.3050e-03, -4.0369e-04],\n",
      "        [ 7.6743e-04, -1.3451e-04],\n",
      "        [ 4.3595e-04, -7.5161e-05],\n",
      "        [ 1.0926e-03, -1.9202e-04],\n",
      "        [ 1.8286e-03, -3.2076e-04],\n",
      "        [ 1.0673e-03, -1.8645e-04],\n",
      "        [ 2.5721e-03, -4.4465e-04],\n",
      "        [ 9.7589e-03, -1.7160e-03],\n",
      "        [ 1.3630e-02, -2.3707e-03],\n",
      "        [ 4.1603e-03, -7.2152e-04],\n",
      "        [ 1.9544e-03, -3.3704e-04],\n",
      "        [ 4.2755e-03, -7.4673e-04],\n",
      "        [ 2.5236e-03, -4.3972e-04],\n",
      "        [ 1.2949e-03, -2.2536e-04],\n",
      "        [ 2.7122e-03, -4.7215e-04],\n",
      "        [ 9.7589e-03, -1.7160e-03],\n",
      "        [ 1.4591e-02, -2.5479e-03],\n",
      "        [ 5.5343e-03, -9.6709e-04],\n",
      "        [ 3.5269e-03, -6.0937e-04],\n",
      "        [ 1.0131e-02, -1.7808e-03],\n",
      "        [ 1.8243e-02, -3.2033e-03],\n",
      "        [ 1.1689e-02, -2.0531e-03],\n",
      "        [ 2.9666e-02, -5.2249e-03],\n",
      "        [ 1.1880e-01, -2.1194e-02]], grad_fn=<ViewAsRealBackward0>)\n",
      "tensor([[ 0.4591,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0192,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0060,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0192,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0192,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [ 0.0060,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0192,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [ 0.0113,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0110,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [-0.0128,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0221,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [ 0.0247,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [-0.0137,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.0246,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [ 0.0375,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0564,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.0274,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [ 0.0378,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [-0.0607,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [ 0.0404,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.0774,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [-0.1292,  0.0000],\n",
      "        [ 0.4591,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "real_imag = torch.view_as_real(psi_predicted)\n",
    "psi_true_real_imag = torch.view_as_real(psi_true.to(torch.complex64))\n",
    "print(real_imag)\n",
    "print(psi_true_real_imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(0.0017, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse = torch.mean((real_imag - psi_true_real_imag) ** 2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "adam.zero_grad()\n",
    "mse.backward(retain_graph=True)\n",
    "adam.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to produce a computational graph provides evidence that .backward actually does full backpropagation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By user ucalyptus, from https://github.com/szagoruyko/pytorchviz/issues/41\n",
    "def resize_graph(dot, size_per_element=0.15, min_size=12):\n",
    "    \"\"\"Resize the graph according to how much content it contains.\n",
    "    Modify the graph in place.\n",
    "    \"\"\"\n",
    "    # Get the approximate number of nodes and edges\n",
    "    num_rows = len(dot.body)\n",
    "    content_size = num_rows * size_per_element\n",
    "    size = max(min_size, content_size)\n",
    "    size_str = str(size) + \",\" + str(size)\n",
    "    dot.graph_attr.update(size=size_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7cf50360b940>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32, 2]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph \u001b[38;5;241m=\u001b[39m make_dot(\n\u001b[1;32m      2\u001b[0m     mse, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()), show_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_saved\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m resize_graph(graph, \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      5\u001b[0m graph\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse_full\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:163\u001b[0m, in \u001b[0;36mmake_dot\u001b[0;34m(var, params, show_attrs, show_saved, max_attr_chars)\u001b[0m\n\u001b[1;32m    161\u001b[0m         add_base_tensor(v)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     add_base_tensor(var)\n\u001b[1;32m    165\u001b[0m resize_graph(dot)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dot\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:151\u001b[0m, in \u001b[0;36mmake_dot.<locals>.add_base_tensor\u001b[0;34m(var, color)\u001b[0m\n\u001b[1;32m    149\u001b[0m dot\u001b[38;5;241m.\u001b[39mnode(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(var)), get_var_name(var), fillcolor\u001b[38;5;241m=\u001b[39mcolor)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (var\u001b[38;5;241m.\u001b[39mgrad_fn):\n\u001b[0;32m--> 151\u001b[0m     add_nodes(var\u001b[38;5;241m.\u001b[39mgrad_fn)\n\u001b[1;32m    152\u001b[0m     dot\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(var\u001b[38;5;241m.\u001b[39mgrad_fn)), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(var)))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var\u001b[38;5;241m.\u001b[39m_is_view():\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:134\u001b[0m, in \u001b[0;36mmake_dot.<locals>.add_nodes\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m u[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m             dot\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(u[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(fn)))\n\u001b[0;32m--> 134\u001b[0m             add_nodes(u[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# note: this used to show .saved_tensors in pytorch0.2, but stopped\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# working* as it was moved to ATen and Variable-Tensor merged\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# also note that this still works for custom autograd functions\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_tensors\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:134\u001b[0m, in \u001b[0;36mmake_dot.<locals>.add_nodes\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m u[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m             dot\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(u[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(fn)))\n\u001b[0;32m--> 134\u001b[0m             add_nodes(u[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# note: this used to show .saved_tensors in pytorch0.2, but stopped\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# working* as it was moved to ATen and Variable-Tensor merged\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# also note that this still works for custom autograd functions\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_tensors\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "    \u001b[0;31m[... skipping similar frames: make_dot.<locals>.add_nodes at line 134 (14 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:134\u001b[0m, in \u001b[0;36mmake_dot.<locals>.add_nodes\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m u[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m             dot\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(u[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(fn)))\n\u001b[0;32m--> 134\u001b[0m             add_nodes(u[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# note: this used to show .saved_tensors in pytorch0.2, but stopped\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# working* as it was moved to ATen and Variable-Tensor merged\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# also note that this still works for custom autograd functions\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_tensors\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torchviz/dot.py:106\u001b[0m, in \u001b[0;36mmake_dot.<locals>.add_nodes\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(SAVED_PREFIX):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, attr)\n\u001b[1;32m    107\u001b[0m seen\u001b[38;5;241m.\u001b[39madd(val)\n\u001b[1;32m    108\u001b[0m attr \u001b[38;5;241m=\u001b[39m attr[\u001b[38;5;28mlen\u001b[39m(SAVED_PREFIX):]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [32, 2]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "graph = make_dot(\n",
    "    mse, params=dict(model.named_parameters()), show_attrs=True, show_saved=True\n",
    ")\n",
    "resize_graph(graph, 0.7)\n",
    "graph.render(\"mse_full\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
